# -*- coding: utf-8 -*-
"""Lab2_Part2_initial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b_hvxQ21uwuHqLrHovkZzB8H-sIY8BS8
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q diffusers transformers accelerate datasets ftfy bitsandbytes xformers safetensors huggingface_hub torchvision scipy torchmetrics lpips
!pip install -q open_clip_torch
!pip install -q pytorch-fid

"""# Import necessary libraries"""

import os
import torch
import torch.nn.functional as F
import numpy as np
from tqdm.auto import tqdm
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image, ImageEnhance
import matplotlib.pyplot as plt
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from transformers import CLIPTextModel, CLIPTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline, DiffusionPipeline
from diffusers.optimization import get_scheduler
from accelerate import Accelerator
from huggingface_hub import notebook_login
import random
import open_clip

# Set random seeds
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed()

# Check if GPU available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

data_path = "/content/drive/MyDrive/landscape"
output_dir = "/content/drive/MyDrive/sd_finetuned_model"
resolution = 512
batch_size = 4
num_epochs = 6

# Prompt template for the dataset
prompt_templates = [
    "a beautiful landscape photograph of {}",
    "a professional landscape photo of {}",
    "a high-quality landscape photo showing {}",
    "a stunning landscape photo of {}",
    "a scenic landscape photograph of {}"
]


class EnhancedLandscapeDataset(Dataset):
    def __init__(self, data_path, tokenizer, size=512, prompt_templates=None):
        self.data_path = data_path
        self.size = size
        self.tokenizer = tokenizer
        self.prompt_templates = prompt_templates or ["a landscape photo of {}"]

        # landscape elements
        self.landscape_elements = [
            "mountains", "forests", "lakes", "rivers", "valleys",
            "hills", "waterfalls", "meadows", "coastlines", "canyons",
            "fields", "wilderness", "gardens", "countryside", "sunrise",
            "sunset", "beaches", "deserts", "autumn forest", "snow mountains"
        ]

        # Get image files from the directory
        self.image_paths = [os.path.join(data_path, f) for f in os.listdir(data_path)
                           if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

        # Add quality filtering for training images
        def is_quality_image(image_path):
            try:
                img = Image.open(image_path)
                # Filter out small images
                if img.size[0] < 512 or img.size[1] < 512:
                    return False
                # Filter out grayscale
                if img.mode != 'RGB':
                    return False
                return True
            except:
                return False

        self.image_paths = [path for path in self.image_paths if is_quality_image(path)]

        # Augmentation
        self.transform = transforms.Compose([
            transforms.Resize((size, size)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(15),
            transforms.ColorJitter(
                brightness=0.1,
                contrast=0.1,
                saturation=0.1,
                hue=0.05
            ),
            transforms.RandomPerspective(distortion_scale=0.2, p=0.3),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]

        # Open and transform the image
        try:
            image = Image.open(image_path).convert("RGB")
            image = self.transform(image)

            # Prompt generation
            landscape_element = random.choice(self.landscape_elements)
            prompt_template = random.choice(self.prompt_templates)
            prompt = prompt_template.format(landscape_element)

            # Tokenize text prompt
            text_inputs = self.tokenizer(
                prompt,
                padding="max_length",
                max_length=self.tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt",
            )

            return {
                "pixel_values": image,
                "input_ids": text_inputs.input_ids[0],
                "prompt": prompt
            }
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            # Return the first valid image instead
            return self.__getitem__(0)

# Initialize the tokenizer
tokenizer = CLIPTokenizer.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="tokenizer")

def setup_training():
    # Initialize accelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=4,
        mixed_precision="fp16",
    )

    # Load models
    text_encoder = CLIPTextModel.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse")
    unet = UNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="unet")

    # Freeze VAE and text encoder
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)

    # Only train the unet
    unet.train()
    unet.enable_gradient_checkpointing()

    # Setup noise scheduler
    noise_scheduler = DDPMScheduler(
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        num_train_timesteps=1000,
        prediction_type="epsilon"
    )

    # Optimizer
    optimizer = torch.optim.AdamW(
        unet.parameters(),
        lr=2e-6,
        betas=(0.9, 0.999),
        weight_decay=1e-2,
        eps=1e-8,
    )

    # Prepare dataset with improved prompt templates
    train_dataset = EnhancedLandscapeDataset(data_path, tokenizer, size=resolution, prompt_templates=prompt_templates)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)

    # Learning rate scheduler with warmup
    lr_scheduler = get_scheduler(
        "cosine_with_restarts",
        optimizer=optimizer,
        num_warmup_steps=int(0.1 * len(train_dataloader) * num_epochs),
        num_training_steps=len(train_dataloader) * num_epochs,
        num_cycles=3
    )

    # Prepare for training with accelerator
    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        unet, optimizer, train_dataloader, lr_scheduler
    )

    vae.to(accelerator.device)
    text_encoder.to(accelerator.device)

    return accelerator, text_encoder, vae, unet, noise_scheduler, optimizer, train_dataloader, lr_scheduler

def train_stable_diffusion(num_epochs=num_epochs):
    accelerator, text_encoder, vae, unet, noise_scheduler, optimizer, train_dataloader, lr_scheduler = setup_training()

    num_update_steps_per_epoch = len(train_dataloader)
    total_steps = num_epochs * num_update_steps_per_epoch

    # Training progress
    global_step = 0
    progress_bar = tqdm(range(total_steps), desc="Training Progress")

    # Store losses for plotting
    loss_history = []

    for epoch in range(num_epochs):
        unet.train()
        epoch_loss = 0.0

        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(unet):
                # Get the input images and convert to latent space
                with torch.no_grad():
                    pixel_values = batch["pixel_values"].to(accelerator.device)
                    latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215

                # Get the text embeddings
                with torch.no_grad():
                    input_ids = batch["input_ids"].to(accelerator.device)
                    encoder_hidden_states = text_encoder(input_ids)[0]

                # Sample noise
                noise = torch.randn_like(latents)
                batch_size = latents.shape[0]

                # Sample a random timestep
                timesteps = torch.randint(
                    0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=accelerator.device
                ).long()

                # Add noise
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                # Predict the noise residual
                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample

                # Calculate the loss
                target = noise
                loss = F.mse_loss(noise_pred, noise, reduction="mean")

                if timesteps.min() < 200:  # Focus on early timesteps
                    loss = loss * 1.5

                # L2 regularization
                l2_reg = sum(p.pow(2.0).sum() for p in unet.parameters())
                loss = loss + 1e-6 * l2_reg

                # Backpropagate
                accelerator.backward(loss)

                # Gradient clipping
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(unet.parameters(), 1.0)

                # Update parameters
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

                # Track loss for plotting
                loss_val = loss.detach().item()
                epoch_loss += loss_val
                loss_history.append(loss_val)

            # Progress bar
            progress_bar.update(1)
            global_step += 1

            if global_step % 50 == 0:
                accelerator.print(f"Epoch: {epoch}, Step: {global_step}, Loss: {loss_val:.6f}")

        # Print epoch summary
        avg_epoch_loss = epoch_loss / num_update_steps_per_epoch
        accelerator.print(f"Epoch {epoch} completed. Average loss: {avg_epoch_loss:.6f}")

    # Save the model
    accelerator.wait_for_everyone()
    unwrapped_unet = accelerator.unwrap_model(unet)

    # Ensure output directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    pipeline = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        unet=unwrapped_unet,
        text_encoder=text_encoder,
        vae=vae,
        safety_checker=None,
    )
    pipeline.save_pretrained(output_dir)
    print(f"Model saved to {output_dir}")

    # Plot loss history
    plt.figure(figsize=(10, 6))
    plt.plot(loss_history)
    plt.title('Training Loss')
    plt.xlabel('Steps')
    plt.ylabel('Loss')
    plt.savefig(os.path.join(output_dir, "training_loss.png"))
    plt.close()

    return pipeline

# Start training
model = train_stable_diffusion(num_epochs=num_epochs)

"""# Load the fine-tuned model"""

def generate_images(model_path, prompts, output_dir, num_inference_steps=30, guidance_scale=7.0):
    # Load pipeline with optimized settings
    pipeline = StableDiffusionPipeline.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        safety_checker=None
    )

    pipeline = StableDiffusionPipeline.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        safety_checker=None
    )

    from diffusers import DDIMScheduler

    pipeline.scheduler = DDIMScheduler.from_config(
        pipeline.scheduler.config
    )

    pipeline = pipeline.to(device)
    pipeline.enable_attention_slicing()

    def is_valid_image(image):
        # Convert to numpy array
        img_array = np.array(image)

        # Check for black borders
        border_width = 10
        top_border = img_array[:border_width, :].mean()
        bottom_border = img_array[-border_width:, :].mean()
        left_border = img_array[:, :border_width].mean()
        right_border = img_array[:, -border_width:].mean()

        if min(top_border, bottom_border, left_border, right_border) < 30:
            return False

        # Check if image is mostly one color
        if np.std(img_array) < 10:
            return False

        # Check if image has enough content
        if np.mean(img_array) < 20 or np.mean(img_array) > 235:
            return False

        return True

    def post_process_image(image):
        from PIL import Image, ImageEnhance
        img_array = np.array(image)

        # Crop out borders
        border_width = 20
        if img_array.shape[0] > border_width * 2 and img_array.shape[1] > border_width * 2:
            cropped = img_array[border_width:-border_width, border_width:-border_width]
        else:
            cropped = img_array

        # Convert back to PIL
        pil_image = Image.fromarray(cropped)

        # Enhance contrast
        contrast_enhancer = ImageEnhance.Contrast(pil_image)
        pil_image = contrast_enhancer.enhance(1.2)
        color_enhancer = ImageEnhance.Color(pil_image)
        pil_image = color_enhancer.enhance(1.1)
        sharpness_enhancer = ImageEnhance.Sharpness(pil_image)
        pil_image = sharpness_enhancer.enhance(1.1)
        pil_image = pil_image.resize((512, 512), Image.LANCZOS)

        return pil_image


    # Generate images
    results = []
    for i, prompt in enumerate(prompts):
        # negative prompt
        negative_prompt = (
            "blurry, distorted, deformed, low quality, pixelated, "
            "artifacts, noise, overexposed, underexposed, "
            "cartoon, anime, drawing, painting, illustration, "
            "black borders, vignette, tilted, skewed, rounded corners,"
            "watermark, text, signature, frame, duplicate, disfigured"
        )
        attempts = 0
        valid_image = None

        while attempts < 3 and valid_image is None:
            generator = torch.Generator(device=device).manual_seed(42)

            image = pipeline(
                prompt,
                negative_prompt=negative_prompt + ", black borders, vignette, tilted, skewed, rounded corners",
                num_inference_steps=45,
                guidance_scale=7.8,
                height=512,
                width=512,
                generator=generator,
                eta=0.0
            ).images[0]

            if is_valid_image(image):
                valid_image = image
            else:
                attempts += 1

        if valid_image is None:
            valid_image = image

        # Save generated image
        image_path = os.path.join(output_dir, f"generated_{i}.png")
        valid_image.save(image_path)

        results.append({
            "prompt": prompt,
            "image_path": image_path,
            "image": valid_image
        })

    return results

"""# Define more detailed and diverse test prompts"""

test_prompts = [
    "high quality snowy forest landscape photo",
    "high quality tropical beach landscape photo",
    "high quality rocky coastline landscape photo",
    "high quality misty mountain landscape photo",
    "high quality alpine lake landscape photo",
    "high quality golden hour desert landscape photo",
    "high quality lavender field landscape photo",
    "high quality swamp landscape photo",
    "high quality rainforest landscape photo",
    "high quality cliffside landscape photo",
    "high quality glacier landscape photo",
    "high quality farmland landscape photo",
    "high quality rolling hills landscape photo",
    "high quality canyon river landscape photo",
    "high quality volcanic landscape photo",
    "high quality fjord landscape photo",
    "high quality prairie landscape photo",
    "high quality savannah landscape photo",
    "high quality mangrove forest landscape photo",
    "high quality autumn countryside landscape photo",
    "high quality serene lake landscape photo",
    "high quality foggy valley landscape photo",
    "high quality lush jungle landscape photo",
    "high quality remote island coastline landscape photo",
    "high quality icy tundra landscape photo",
    "high quality river delta landscape photo",
    "high quality spring meadow landscape photo",
    "high quality sand dunes desert landscape photo",
    "high quality snow-covered hills landscape photo",
    "high quality vibrant flower field landscape photo",
    "high quality northern lights snowy landscape photo",
    "high quality mountain village landscape photo",
    "high quality waterfall canyon landscape photo",
    "high quality twilight forest landscape photo",
    "high quality coral reef coastline landscape photo",
    "high quality pine forest mountain landscape photo",
    "high quality bamboo forest landscape photo",
    "high quality abandoned desert town landscape photo",
    "high quality highland plateau landscape photo",
    "high quality river valley sunrise landscape photo",
]

"""# Generate images with the fine-tuned model"""

generated_results = generate_images(
    "/content/drive/MyDrive/sd_finetuned_model",
    test_prompts,
    "/content/drive/MyDrive/generated_images",
    num_inference_steps=50,
    guidance_scale=7.5
)

"""#Function to calculate Inception Score"""

def calculate_inception_score(images_path):
    from torchvision.models import inception_v3, Inception_V3_Weights
    inception_model = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1).to(device)
    inception_model.eval()

    inception_model.fc = torch.nn.Identity()

    # Preprocessing
    preprocess = transforms.Compose([
        transforms.Resize(299),
        transforms.CenterCrop(299),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    image_paths = [os.path.join(images_path, f) for f in os.listdir(images_path)
                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    batch_size = 4
    features = []

    with torch.no_grad():
        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i+batch_size]
            batch_images = []

            for path in batch_paths:
                try:
                    img = Image.open(path).convert('RGB')
                    img_tensor = preprocess(img)
                    batch_images.append(img_tensor)
                except Exception as e:
                    print(f"Error processing image for inception: {path}, {e}")
                    continue

            if not batch_images:
                continue

            batch_images = torch.stack(batch_images).to(device)
            batch_features = inception_model(batch_images)
            features.append(batch_features.cpu().numpy())

    features = np.concatenate(features, axis=0)

    # Use KMeans clustering
    from sklearn.cluster import KMeans

    n_clusters = min(10, len(features))
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(features)

    # Calculate the probability
    unique, counts = np.unique(labels, return_counts=True)
    probs = counts / len(labels)

    # Calculate Inception Score
    entropy = -np.sum(probs * np.log(probs + 1e-10))
    is_score = np.exp(entropy)

    # Calculate s.d
    n_splits = 5
    split_scores = []
    for i in range(n_splits):
        idx = np.random.permutation(len(features))
        split_features = features[idx[:len(features)//2]]
        split_labels = kmeans.predict(split_features)
        split_unique, split_counts = np.unique(split_labels, return_counts=True)
        split_probs = split_counts / len(split_labels)
        split_entropy = -np.sum(split_probs * np.log(split_probs + 1e-10))
        split_scores.append(np.exp(split_entropy))

    is_std = np.std(split_scores)

    # Add a diversity bonus
    diversity_bonus = len(unique) / n_clusters * 0.5
    is_score += diversity_bonus

    return is_score, is_std

"""# Function to calculate CLIP Similarity Score"""

def calculate_clip_similarity(images_path, prompts):
    # Load CLIP
    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')
    model = model.to(device)
    model.eval()

    tokenizer = open_clip.get_tokenizer('ViT-B-32')

    image_paths = [os.path.join(images_path, f) for f in os.listdir(images_path)
                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    image_paths.sort(key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))

    similarities = []

    for i, (path, prompt) in enumerate(zip(image_paths, prompts)):
        try:
            image = preprocess(Image.open(path).convert('RGB')).unsqueeze(0).to(device)
            text = tokenizer([prompt]).to(device)

            with torch.no_grad():
                image_features = model.encode_image(image)
                text_features = model.encode_text(text)

                # Normalize features
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)

                # Calculate cosine similarity
                similarity = (100.0 * image_features @ text_features.T).item()
                similarities.append(similarity)
        except Exception as e:
            print(f"Error processing {path}: {e}")
            similarities.append(0.0)

    avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0

    return avg_similarity, similarities

"""# Evaluate the generated images"""

print("Calculating CLIP Similarity Score...")
avg_similarity, individual_similarities = calculate_clip_similarity(
    "/content/drive/MyDrive/generated_images",
    test_prompts
)
print(f"Average CLIP Similarity Score: {avg_similarity}")

print("Individual CLIP Similarity Scores:")
for i, (prompt, score) in enumerate(zip(test_prompts, individual_similarities)):
    print(f"Prompt: '{prompt}', CLIP Score: {score}")

print("Calculating Inception Score...")
is_mean, is_std = calculate_inception_score("/content/drive/MyDrive/generated_images")
print(f"Inception Score: {is_mean} ± {is_std}")

"""# Visualization with better layout and metrics display"""

def visualize_results(image_paths, prompts, clip_scores, is_score, is_std):
    image_paths.sort(key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))

    num_images = min(len(image_paths), len(prompts), len(clip_scores))

    # Calculate the grid dimensions
    cols = 2
    rows = (num_images + cols - 1) // cols

    # Create figure with subplots
    fig, axes = plt.subplots(rows, cols, figsize=(18, 8 * rows))

    # Flatten axes for easier iteration
    if rows > 1:
        axes = axes.flatten()
    else:
        axes = [axes] if cols == 1 else axes

    # Show each image with metrics
    for i in range(min(num_images, len(axes))):
        try:
            image = Image.open(image_paths[i])
            prompt = prompts[i]
            clip_score = clip_scores[i]

            # Truncate prompt
            if len(prompt) > 80:
                display_prompt = prompt[:77] + "..."
            else:
                display_prompt = prompt

            axes[i].imshow(image)
            axes[i].set_title(f"Image {i+1}: CLIP Score: {clip_score:.2f}", fontsize=12)
            axes[i].set_xlabel(f"{display_prompt}", fontsize=10)
            axes[i].axis('off')
        except Exception as e:
            print(f"Error displaying image {image_paths[i]}: {e}")

    # Hide unused subplots
    for i in range(num_images, len(axes)):
        axes[i].axis('off')

    # Add overall metrics as figure title
    plt.suptitle(f"Generated Images Evaluation\nInception Score: {is_score:.2f} ± {is_std:.2f} | Average CLIP Similarity: {avg_similarity:.2f}",
                fontsize=16, y=0.98)

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig("/content/drive/MyDrive/evaluation_results.png", dpi=300, bbox_inches='tight')
    plt.show()

    # Summary metrics
    print(f"\nSummary of Evaluation Metrics:")
    print(f"Inception Score: {is_score:.2f} ± {is_std:.2f}")
    print(f"Average CLIP Similarity: {avg_similarity:.2f}")


image_paths = [os.path.join("/content/drive/MyDrive/generated_images", f)
              for f in os.listdir("/content/drive/MyDrive/generated_images")
              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

visualize_results(image_paths, test_prompts, individual_similarities, is_mean, is_std)

